{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "4ee94787-26b0-4063-9236-685cee44f84f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import requests\n",
    "import gzip\n",
    "import pgeocode\n",
    "from meteostat import Point, Hourly\n",
    "import datetime\n",
    "import pathlib\n",
    "import holidays\n",
    "import zoneinfo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "4ac3099e-53f6-4065-9c4d-9a3f73dc4790",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Created or already existed: data/complete_dfs\n",
      "Created or already existed: data/pjm\n",
      "Created or already existed: data/weather\n",
      "Created or already existed: models\n"
     ]
    }
   ],
   "source": [
    "def getWeatherFilePath(load_area):\n",
    "    filepath = \"data/weather/\" + load_area + \".csv\"\n",
    "    return filepath\n",
    "\n",
    "def getPjmFilePath(year):\n",
    "    filepath = \"data/pjm/\" + \"hrl_load_metered_\" + str(year) + \".csv\"\n",
    "    return filepath\n",
    "\n",
    "def getPjmFreshFilePath():\n",
    "    return \"data/pjm/hrl_load_metered_fresh.csv\"\n",
    "\n",
    "def getCompleteDfFilePath(load_area):\n",
    "    filepath = \"data/complete_dfs/\" + load_area + \".csv\"\n",
    "    return filepath\n",
    "\n",
    "def getModelFilePath(load_area):\n",
    "    filepath = \"models/\" + load_area + \".pkl\"\n",
    "\n",
    "def makeDirectories():\n",
    "    base = pathlib.Path(\"data\")\n",
    "    subdirs = [\"complete_dfs\", \"pjm\", \"weather\"]\n",
    "    for sub in subdirs:\n",
    "        path = base / sub\n",
    "        path.mkdir(parents=True, exist_ok=True)\n",
    "        print(f\"Created or already existed: {path}\")\n",
    "    path = pathlib.Path(\"models\")\n",
    "    path.mkdir(parents=True, exist_ok=True)\n",
    "    print(f\"Created or already existed: {path}\")\n",
    "\n",
    "makeDirectories()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "ac4b9608-161c-48b4-9c73-d417dab88111",
   "metadata": {},
   "outputs": [],
   "source": [
    "def getLoadAreaToZips():\n",
    "\n",
    "    # RTO is the *entire PJM footprint*, not a load zone. no meaningful ZIP.\n",
    "    mymap = {\n",
    "        # Atlantic City Electric (Southern New Jersey)\n",
    "        'AECO': ['08401'],   # Atlantic City, NJ\n",
    "    \n",
    "        # American Electric Power - Appalachian Power (central and Southern West Virginia)\n",
    "        'AEPAPT': ['25301'],   # Charleston, WV\n",
    "\n",
    "        # American Electric Power - Indiana Michigan Power (northeast quadrant of indiana and southwest corner of michigan)\n",
    "        'AEPIMP': ['46802'],   # Fort Wayne, IN\n",
    "\n",
    "        # American Electric Power - Kentucky Power (eastern kentucky)\n",
    "        'AEPKPT': ['41101'],   # Ashland, KY (Eastern Kentucky Power region)\n",
    "\n",
    "        # American Electric Power - Ohio (central and southeast ohio)\n",
    "        'AEPOPT': ['43215'],   # Columbus, OH\n",
    "\n",
    "        # Allegheny Power (FirstEnergy West) serving MD/WV/PA panhandle\n",
    "        'AP': ['21502'],     # Cumberland, MD\n",
    "\n",
    "        # Baltimore Gas & Electric\n",
    "        'BC': ['21201'],     # Baltimore, MD (city center)\n",
    "\n",
    "        # Cleveland Electric Illuminating Company (FirstEnergy)\n",
    "        'CE': ['44114'],     # Cleveland, OH (downtown)\n",
    "\n",
    "        # Dayton Power & Light (AES Ohio)\n",
    "        'DAY': ['45402'],    # Dayton, OH\n",
    "\n",
    "        # Duke Energy Ohio/Kentucky load zone\n",
    "        'DEOK': ['45202'],   # Cincinnati, OH\n",
    "\n",
    "        # Dominion Virginia Power\n",
    "        'DOM': ['23219'],    # Richmond, VA\n",
    "\n",
    "        # Delmarva Power (Delaware & Eastern Shore MD)\n",
    "        'DPLCO': ['19901'],  # Dover, DE\n",
    "\n",
    "        # Duquesne Light\n",
    "        'DUQ': ['15222'],    # Pittsburgh, PA\n",
    "\n",
    "        # Easton Utilities (Maryland municipal)\n",
    "        'EASTON': ['21601'], # Easton, MD\n",
    "\n",
    "        # East Kentucky Power Cooperative\n",
    "        'EKPC': ['40391'],   # Winchester, KY\n",
    "\n",
    "        # Jersey Central Power & Light (FirstEnergy NJ)\n",
    "        'JC': ['07728'],     # Freehold, NJ\n",
    "\n",
    "        # Metropolitan Edison (FirstEnergy PA)\n",
    "        'ME': ['19601'],     # Reading, PA\n",
    "\n",
    "        # Ohio Edison (FirstEnergy OH)\n",
    "        'OE': ['44308'],     # Akron, OH\n",
    "\n",
    "        # Ohio Valley Electric Corporation\n",
    "        'OVEC': ['45661'],   # Piketon, OH\n",
    "\n",
    "        # Pennsylvania Power Company (FirstEnergy PA)\n",
    "        'PAPWR': ['16101'],  # New Castle, PA\n",
    "\n",
    "        # PECO (Philadelphia)\n",
    "        'PE': ['19103'],     # Philadelphia, PA (Center City)\n",
    "\n",
    "        # Potomac Electric Power Company (Washington DC + Montgomery Co MD)\n",
    "        'PEPCO': ['20001'],  # Washington, DC\n",
    "\n",
    "        # Potomac Edison (FirstEnergy MD/WV)\n",
    "        'PLCO': ['21740'],   # Hagerstown, MD\n",
    "\n",
    "        # Pennsylvania Electric Company (Penelec - FirstEnergy Northwest/Central PA)\n",
    "        'PN': ['16601'],     # Altoona, PA\n",
    "\n",
    "        # Public Service Electric & Gas (PSE&G NJ)\n",
    "        'PS': ['07102'],     # Newark, NJ\n",
    "\n",
    "        # Rockland Electric (Northern NJ / small NY portion)\n",
    "        'RECO': ['07450'],   # Ridgewood, NJ\n",
    "\n",
    "        # Southern Maryland Electric Cooperative\n",
    "        'SMECO': ['20650'],  # Leonardtown, MD\n",
    "\n",
    "        # UGI Electric (NE Pennsylvania, Luzerne County)\n",
    "        'UGI': ['18702'],    # Wilkes-Barre, PA\n",
    "\n",
    "        # VMEU = Virginia Municipal Electric Utility (multiple small cities)\n",
    "        'VMEU': ['22801']    # Harrisonburg, VA (representative muni)\n",
    "    }\n",
    "    return mymap\n",
    "\n",
    "# Returns the id of the load area (1-indexed)\n",
    "def getLoadAreaId(load_area):\n",
    "    load_areas = getLoadAreaToZips().keys()\n",
    "    load_areas = sorted(load_areas)\n",
    "    return load_areas.index(load_area)+1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "0e712946-e0cc-4f56-88af-d4d864aac759",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2016 did not match! The difference is: {'VMEU', 'OVEC', 'AECO', 'AE'}\n",
      "2017 did not match! The difference is: {'OVEC'}\n",
      "Finished checking inconsistencies!\n"
     ]
    }
   ],
   "source": [
    "def checkInconsistencies():\n",
    "    load_area_to_zips = getLoadAreaToZips()\n",
    "    for year in range(2016,2026):\n",
    "        df = pd.read_csv(getPjmFilePath(year))\n",
    "        #print(df['zone'].unique())\n",
    "        #print(df['load_area'].unique())\n",
    "        #print(len(df['load_area'].unique()))\n",
    "        set1 = set(df['load_area'].unique())\n",
    "        set2 = set(load_area_to_zips.keys())\n",
    "        set2.add(\"RTO\")\n",
    "        # Note that years 2016 and 2017 do not match. Thus, we will start with 2018 inclusive\n",
    "        if len(set1.symmetric_difference(set2)) > 0:\n",
    "            print(str(year) + \" did not match! The difference is: \" + str(set1.symmetric_difference(set2)))\n",
    "    \n",
    "    print(\"Finished checking inconsistencies!\")\n",
    "\n",
    "checkInconsistencies()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "c64fff20-540f-4fcb-95db-dce506a1c8b5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cached weather file for AECO was found\n",
      "Cached weather file for AEPAPT was found\n",
      "Cached weather file for AEPIMP was found\n",
      "Cached weather file for AEPKPT was found\n",
      "Cached weather file for AEPOPT was found\n",
      "Cached weather file for AP was found\n",
      "Added new cached weather file for BC using zip code:21201\n",
      "Cached weather file for CE was found\n",
      "Cached weather file for DAY was found\n",
      "Cached weather file for DEOK was found\n",
      "Cached weather file for DOM was found\n",
      "Cached weather file for DPLCO was found\n",
      "Cached weather file for DUQ was found\n",
      "Cached weather file for EASTON was found\n",
      "Cached weather file for EKPC was found\n",
      "Cached weather file for JC was found\n",
      "Cached weather file for ME was found\n",
      "Cached weather file for OE was found\n",
      "Cached weather file for OVEC was found\n",
      "Cached weather file for PAPWR was found\n",
      "Cached weather file for PE was found\n",
      "Cached weather file for PEPCO was found\n",
      "Cached weather file for PLCO was found\n",
      "Cached weather file for PN was found\n",
      "Cached weather file for PS was found\n",
      "Cached weather file for RECO was found\n",
      "Cached weather file for SMECO was found\n",
      "Cached weather file for UGI was found\n",
      "Cached weather file for VMEU was found\n",
      "Cached weather file for AECO was found\n",
      "                     temp  dwpt  rhum  prcp  snow  wdir  wspd  wpgt  pres  \\\n",
      "time                                                                        \n",
      "2018-01-01 01:00:00 -12.5 -16.2  74.0   NaN   NaN   0.0   0.0   NaN   NaN   \n",
      "2018-01-01 02:00:00 -14.1 -16.9  79.0   NaN   NaN   0.0   0.0   NaN   NaN   \n",
      "2018-01-01 03:00:00 -14.6 -17.4  79.0   NaN   NaN   0.0   0.0   NaN   NaN   \n",
      "2018-01-01 04:00:00 -15.5 -18.2  80.0   NaN   NaN   0.0   0.0   NaN   NaN   \n",
      "2018-01-01 05:00:00 -15.2 -18.0  79.0   NaN   NaN   0.0   0.0   NaN   NaN   \n",
      "\n",
      "                     tsun  coco  \n",
      "time                             \n",
      "2018-01-01 01:00:00   NaN   NaN  \n",
      "2018-01-01 02:00:00   NaN   NaN  \n",
      "2018-01-01 03:00:00   NaN   NaN  \n",
      "2018-01-01 04:00:00   NaN   NaN  \n",
      "2018-01-01 05:00:00   NaN   NaN  \n"
     ]
    }
   ],
   "source": [
    "def getYears():\n",
    "    return range(2018,2026)\n",
    "\n",
    "def getWeatherDf(load_area, force_refresh=False):\n",
    "    # Create a Nominatim geocoder instance for the desired country (For the USA, use 'us')\n",
    "    nomi = pgeocode.Nominatim('us')\n",
    "    years = getYears()\n",
    "    # First check if we have the information cached\n",
    "    file_path = getWeatherFilePath(load_area)\n",
    "    if os.path.exists(file_path) and not force_refresh:\n",
    "        temp = pd.read_csv(file_path)\n",
    "        #print(temp.head())\n",
    "        temp['time'] = pd.to_datetime(temp['time'])\n",
    "        temp = temp.set_index('time')\n",
    "        #print(temp.head())\n",
    "        if temp.index.min().year == years[0] and temp.index.max().year == years[-1]:\n",
    "            print(\"Cached weather file for \" + load_area + \" was found\")\n",
    "            return temp\n",
    "\n",
    "    # Query the postal code\n",
    "    load_area_to_zips = getLoadAreaToZips()\n",
    "    zip_code = load_area_to_zips[load_area][0]\n",
    "    location_data = nomi.query_postal_code(zip_code)\n",
    "    latitude = float(location_data.latitude)\n",
    "    longitude = float(location_data.longitude)\n",
    "    #print(latitude)\n",
    "    #print(longitude)\n",
    "\n",
    "    location = Point(latitude, longitude)\n",
    "    start = datetime.datetime(years[0], 1, 1)\n",
    "    end   = datetime.datetime(years[-1], 12, 31)\n",
    "\n",
    "    data = Hourly(location, start, end).fetch()\n",
    "    #print(data.head())\n",
    "    data.to_csv(getWeatherFilePath(load_area))\n",
    "    print(\"Added new cached weather file for \" + load_area + \" using zip code:\" + str(zip_code))\n",
    "    return data\n",
    "\n",
    "def getAllWeatherDfs():\n",
    "    load_area_to_zips = getLoadAreaToZips()\n",
    "    for key, value in load_area_to_zips.items():\n",
    "        load_area = key\n",
    "        getWeatherDf(load_area)\n",
    "\n",
    "getAllWeatherDfs()\n",
    "aeco_weather_df = getWeatherDf(\"AECO\")\n",
    "print(aeco_weather_df.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "9fbd4772-f843-4f25-8d35-a1eaf69a0aeb",
   "metadata": {},
   "outputs": [],
   "source": [
    "def getUSHolidays():\n",
    "    years = getYears()\n",
    "    us_holidays = holidays.US(years=years)\n",
    "    # Add black friday\n",
    "    for year in years:\n",
    "        thanksgiving = [day for day, name in us_holidays.items() if name == \"Thanksgiving Day\" and day.year == year][0]\n",
    "        us_holidays[thanksgiving + datetime.timedelta(days=1)] = \"Black Friday\"\n",
    "        us_holidays[thanksgiving - datetime.timedelta(days=1)] = \"Thanksgiving Eve\"\n",
    "\n",
    "    return us_holidays\n",
    "\n",
    "#print(us_holidays)\n",
    "#print(us_holidays.keys())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "3ac9f6cb-8c14-47e7-96ff-6b3bce36c9c4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True\n",
      "True\n"
     ]
    }
   ],
   "source": [
    "def isHoliday(datetime_obj, holidays_map):\n",
    "    return datetime_obj.date() in holidays_map\n",
    "\n",
    "def isThanksgiving(datetime_obj, holidays_map):\n",
    "    return holidays_map.get(datetime_obj.date()) == \"Thanksgiving Day\"\n",
    "    \n",
    "def isThanksgivingEve(datetime_obj, holidays_map):\n",
    "    return holidays_map.get(datetime_obj.date()) == \"Thanksgiving Eve\"\n",
    "    \n",
    "def isBlackFriday(datetime_obj, holidays_map):\n",
    "    return holidays_map.get(datetime_obj.date()) == \"Black Friday\"\n",
    "    \n",
    "def isWeekend(datetime_obj):\n",
    "    weekno = datetime.datetime.today().weekday()\n",
    "    if weekno < 5:\n",
    "        return False\n",
    "    return True\n",
    "\n",
    "# Thanksgiving 2024 (Nov 28, 2024)\n",
    "#test_dt = datetime.datetime(2024, 11, 29, 15, 0)  # 3pm on Thanksgiving Day\n",
    "#print(isHoliday(test_dt, getUSHolidays()))\n",
    "#print(isBlackFriday(test_dt, getUSHolidays()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "f6db3b5f-52b8-48c5-a251-3a6afb8026b9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cached pjm fresh file was found\n",
      "Loading fresh data in addition to historical data\n",
      "                       datetime_beginning_ept nerc_region mkt_region zone  \\\n",
      "datetime_beginning_utc                                                      \n",
      "2025-11-10 00:00:00      11/9/2025 7:00:00 PM         RFC     MIDATL   AE   \n",
      "2025-11-10 01:00:00      11/9/2025 8:00:00 PM         RFC     MIDATL   AE   \n",
      "2025-11-10 02:00:00      11/9/2025 9:00:00 PM         RFC     MIDATL   AE   \n",
      "2025-11-10 03:00:00     11/9/2025 10:00:00 PM         RFC     MIDATL   AE   \n",
      "2025-11-10 04:00:00     11/9/2025 11:00:00 PM         RFC     MIDATL   AE   \n",
      "\n",
      "                       load_area       mw  is_verified  \n",
      "datetime_beginning_utc                                  \n",
      "2025-11-10 00:00:00         AECO  980.590        False  \n",
      "2025-11-10 01:00:00         AECO  952.221        False  \n",
      "2025-11-10 02:00:00         AECO  908.209        False  \n",
      "2025-11-10 03:00:00         AECO  855.014        False  \n",
      "2025-11-10 04:00:00         AECO  799.135        False  \n"
     ]
    }
   ],
   "source": [
    "# Load PJM data\n",
    "\n",
    "def getEnergyDf(load_area, force_refresh=False):\n",
    "    years = getYears()\n",
    "    load_area_to_zips = getLoadAreaToZips()\n",
    "    if load_area not in load_area_to_zips.keys():\n",
    "        print(\"Error: load_area(\" + load_area + \") not found!\")\n",
    "        return -1\n",
    "\n",
    "    ret_df = None\n",
    "    for year in years:\n",
    "        pjm_df = pd.read_csv(getPjmFilePath(year))\n",
    "        pjm_df = pjm_df[pjm_df['load_area'] == load_area]\n",
    "        \n",
    "        if ret_df is None:\n",
    "            ret_df = pjm_df\n",
    "        else:\n",
    "            ret_df = pd.concat([ret_df, pjm_df])\n",
    "\n",
    "    # Note: 2025 is hardcoded, but whatever\n",
    "    if 2025 in years:\n",
    "        # first check if it is cached\n",
    "        file_path = getPjmFreshFilePath()\n",
    "        if not os.path.exists(file_path) or force_refresh:\n",
    "            url = \"https://raw.githubusercontent.com/SurtaiHan/stats604fa25proj4/refs/heads/main/data/pjm/hrl_load_metered_fresh.csv\"\n",
    "            out_path = pathlib.Path(getPjmFreshFilePath())\n",
    "            response = requests.get(url)\n",
    "            response.raise_for_status()  # raises if download failed\n",
    "            out_path.write_bytes(response.content)\n",
    "            print(f\"Downloaded fresh pjm csv to: {out_path}\")\n",
    "        else:\n",
    "             print(\"Cached pjm fresh file was found\")\n",
    "        \n",
    "        print(\"Loading fresh data in addition to historical data\")\n",
    "        pjm_df = pd.read_csv(getPjmFreshFilePath())\n",
    "        pjm_df = pjm_df[pjm_df['load_area'] == load_area]\n",
    "        if ret_df is None:\n",
    "            ret_df = pjm_df\n",
    "        else:\n",
    "            ret_df = pd.concat([ret_df, pjm_df])\n",
    "\n",
    "    ret_df['datetime_beginning_utc'] = pd.to_datetime(\n",
    "        ret_df['datetime_beginning_utc'], \n",
    "        #utc=True\n",
    "    )\n",
    "    ret_df = ret_df.set_index('datetime_beginning_utc')\n",
    "    ret_df = ret_df.sort_index()   # Always a good idea after setting index\n",
    "\n",
    "    return ret_df\n",
    "\n",
    "aeco_energy_df = getEnergyDf(\"AECO\")\n",
    "print(aeco_energy_df.tail())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "e337a1c9-cf6d-4e5b-9b74-bd1ac360216a",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Last step is to augment the pjm data with:\n",
    "# 1) weather at that current time  (we just use the ground truth weather)\n",
    "# 2) previous day's weather\n",
    "# 3) isHoliday, isThanksgiving, isThanksgivingEve, isBlackFriday, isWeekend\n",
    "# 4) lagged loads (24 hrs ago, 168 hrs ago)\n",
    "\n",
    "# The resulting dataframe could be viewed as X,y tuples\n",
    "# in the sense that everything but \n",
    "\n",
    "# weather variables available:\n",
    "# temp\tdwpt\trhum\tprcp\tsnow\twdir\twspd\twpgt\tpres\ttsun\tcoco\n",
    "\n",
    "def add_features(energy_df, weather_df, dropna=True, outer=False):\n",
    "    \"\"\"\n",
    "    df must contain at least:\n",
    "    - load (float)\n",
    "    - temp, dwpt, rhum, wspd, tsun, etc. (weather columns)\n",
    "    - index is hourly timestamps (pd.DatetimeIndex)\n",
    "    \"\"\"\n",
    "\n",
    "    \"\"\"df = df.merge(\n",
    "    weather_df[['temp']], # Only select 'temp', as the timestamp is the index\n",
    "    left_on='datetime_beginning_utc',\n",
    "    right_index=True,   # Tells pandas to use the index of weather_df (the DatetimeIndex)\n",
    "    how='left'\n",
    "    )\"\"\"\n",
    "    df = energy_df.copy()\n",
    "\n",
    "    if outer:\n",
    "        df = df.join(weather_df[['temp']], how='outer')\n",
    "    else:\n",
    "        df = df.join(weather_df[['temp']], how='left')\n",
    "    \n",
    "    # --- Calendar Features ---\n",
    "    df['hour'] = df.index.hour\n",
    "    df['dayofweek'] = df.index.dayofweek\n",
    "    df['month'] = df.index.month\n",
    "    df['is_weekend'] = df['dayofweek'].isin([5,6]).astype(int)\n",
    "\n",
    "    # Holiday features (assuming you already defined isHoliday / isThanksgiving):\n",
    "    us_holidays = getUSHolidays()\n",
    "    df['is_holiday'] = df.index.to_series().apply(isHoliday, args=(us_holidays,))\n",
    "    df['is_thanksgiving'] = df.index.to_series().apply(isThanksgiving, args=(us_holidays,))\n",
    "    df['is_thanksgiving_eve'] = df.index.to_series().apply(isThanksgivingEve, args=(us_holidays,))\n",
    "    df['is_black_friday'] = df.index.to_series().apply(isBlackFriday, args=(us_holidays,))\n",
    "\n",
    "    # --- Degree Days ---\n",
    "    df['HDD'] = (18 - df['temp']).clip(lower=0)\n",
    "    df['CDD'] = (df['temp'] - 18).clip(lower=0)\n",
    "\n",
    "    # --- Rolling Weather Averages ---\n",
    "    df['temp_rolling_24'] = df['temp'].rolling('24h', min_periods=1).mean()\n",
    "    df['temp_rolling_48'] = df['temp'].rolling('48h', min_periods=1).mean()\n",
    "    df['temp_rolling_72'] = df['temp'].rolling('72h', min_periods=1).mean()\n",
    "    # df['dwpt_rolling_24'] = df['dwpt'].rolling(24, min_periods=1).mean()\n",
    "\n",
    "    # --- Lagged Weather Features ---\n",
    "    # 24 48 72 96 120 144 168 192 216 240\n",
    "    df['temp_lag_24'] = df['temp'].shift(freq='24h')\n",
    "    df['temp_lag_48'] = df['temp'].shift(freq='48h')\n",
    "    df['temp_lag_72'] = df['temp'].shift(freq='72h')\n",
    "    df['temp_lag_96'] = df['temp'].shift(freq='96h')\n",
    "    df['temp_lag_120'] = df['temp'].shift(freq='120h')\n",
    "    df['temp_lag_168'] = df['temp'].shift(freq='168h')\n",
    "    df['temp_lag_192'] = df['temp'].shift(freq='192h')\n",
    "    df['temp_lag_216'] = df['temp'].shift(freq='216h')\n",
    "    df['temp_lag_240'] = df['temp'].shift(freq='240h')\n",
    "\n",
    "    # --- Lagged Load Features (Key Predictors) ---\n",
    "    # 72 96 120 144 168 192 216 240\n",
    "    df['mw_lag_72'] = df['mw'].shift(freq='72h')\n",
    "    df['mw_lag_96'] = df['mw'].shift(freq='96h')\n",
    "    df['mw_lag_120'] = df['mw'].shift(freq='120h')\n",
    "    df['mw_lag_144'] = df['mw'].shift(freq='144h')\n",
    "    df['mw_lag_168'] = df['mw'].shift(freq='168h')\n",
    "    df['mw_lag_192'] = df['mw'].shift(freq='192h')\n",
    "    df['mw_lag_216'] = df['mw'].shift(freq='216h')\n",
    "    df['mw_lag_240'] = df['mw'].shift(freq='240h')\n",
    "\n",
    "    print(\"Shape before dropna:\", df.shape)\n",
    "    # Drop rows where lag features are missing\n",
    "    if dropna:\n",
    "        df = df.dropna()\n",
    "    print(\"Shape after dropna:\", df.shape)\n",
    "\n",
    "    return df\n",
    "\n",
    "#df_augmented = add_features(aeco_energy_df, aeco_weather_df, False)\n",
    "#print(df_augmented)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "4d9ebbd7-f548-48a6-b624-c9ed431dd53e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cached complete file for AECO was found\n",
      "Cached complete file for AEPAPT was found\n",
      "Cached complete file for AEPIMP was found\n",
      "Cached pjm fresh file was found\n",
      "Loading fresh data in addition to historical data\n",
      "Cached weather file for AEPKPT was found\n",
      "Shape before dropna: (68856, 38)\n",
      "Shape after dropna: (68051, 38)\n",
      "Added new cached complete file for AEPKPT\n",
      "Cached complete file for AEPOPT was found\n",
      "Cached complete file for AP was found\n",
      "Cached complete file for BC was found\n",
      "Cached complete file for CE was found\n",
      "Cached complete file for DAY was found\n",
      "Cached complete file for DEOK was found\n",
      "Cached complete file for DOM was found\n",
      "Cached complete file for DPLCO was found\n",
      "Cached complete file for DUQ was found\n",
      "Cached complete file for EASTON was found\n",
      "Cached complete file for EKPC was found\n",
      "Cached complete file for JC was found\n",
      "Cached complete file for ME was found\n",
      "Cached complete file for OE was found\n",
      "Cached complete file for OVEC was found\n",
      "Cached complete file for PAPWR was found\n",
      "Cached complete file for PE was found\n",
      "Cached complete file for PEPCO was found\n",
      "Cached complete file for PLCO was found\n",
      "Cached complete file for PN was found\n",
      "Cached complete file for PS was found\n",
      "Cached complete file for RECO was found\n",
      "Cached complete file for SMECO was found\n",
      "Cached complete file for UGI was found\n",
      "Cached complete file for VMEU was found\n"
     ]
    }
   ],
   "source": [
    "def getCompleteDf(load_area):\n",
    "    # First check if we have the information cached\n",
    "    file_path = getCompleteDfFilePath(load_area)\n",
    "    if os.path.exists(file_path):\n",
    "        temp = pd.read_csv(file_path)\n",
    "        temp['datetime_beginning_utc'] = pd.to_datetime(temp['datetime_beginning_utc'])\n",
    "        temp = temp.set_index('datetime_beginning_utc')\n",
    "        temp = temp.sort_index()\n",
    "        print(\"Cached complete file for \" + load_area + \" was found\")\n",
    "        return temp\n",
    "\n",
    "    energy_df = getEnergyDf(load_area)\n",
    "    weather_df = getWeatherDf(load_area)\n",
    "    df_augmented = add_features(energy_df, weather_df)\n",
    "    # load_area_to_complete_df[load_area] = df_augmented\n",
    "    df_augmented.to_csv(getCompleteDfFilePath(load_area))\n",
    "    print(\"Added new cached complete file for \" + load_area)\n",
    "    return df_augmented\n",
    "\n",
    "def getAllCompleteDfs():\n",
    "    load_area_2_complete_df = {}\n",
    "    load_area_to_zips = getLoadAreaToZips()\n",
    "    for load_area in load_area_to_zips.keys():\n",
    "        load_area_2_complete_df[load_area] = getCompleteDf(load_area)\n",
    "    return load_area_2_complete_df\n",
    "\n",
    "# print(getCompleteDf(\"AECO\").head())\n",
    "\n",
    "load_area_to_complete_df = getAllCompleteDfs()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "ab5fd67f-92d3-4f13-847f-cdd830738ad3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cached pjm fresh file was found\n",
      "Loading fresh data in addition to historical data\n",
      "Added new cached weather file for AECO using zip code:08401\n",
      "Shape before dropna: (69238, 38)\n",
      "Shape after dropna: (69238, 38)\n",
      "                    datetime_beginning_ept nerc_region mkt_region zone  \\\n",
      "2025-11-15 05:00:00                    NaN         NaN        NaN  NaN   \n",
      "2025-11-15 06:00:00                    NaN         NaN        NaN  NaN   \n",
      "2025-11-15 07:00:00                    NaN         NaN        NaN  NaN   \n",
      "2025-11-15 08:00:00                    NaN         NaN        NaN  NaN   \n",
      "2025-11-15 09:00:00                    NaN         NaN        NaN  NaN   \n",
      "2025-11-15 10:00:00                    NaN         NaN        NaN  NaN   \n",
      "2025-11-15 11:00:00                    NaN         NaN        NaN  NaN   \n",
      "2025-11-15 12:00:00                    NaN         NaN        NaN  NaN   \n",
      "2025-11-15 13:00:00                    NaN         NaN        NaN  NaN   \n",
      "2025-11-15 14:00:00                    NaN         NaN        NaN  NaN   \n",
      "2025-11-15 15:00:00                    NaN         NaN        NaN  NaN   \n",
      "2025-11-15 16:00:00                    NaN         NaN        NaN  NaN   \n",
      "2025-11-15 17:00:00                    NaN         NaN        NaN  NaN   \n",
      "2025-11-15 18:00:00                    NaN         NaN        NaN  NaN   \n",
      "2025-11-15 19:00:00                    NaN         NaN        NaN  NaN   \n",
      "2025-11-15 20:00:00                    NaN         NaN        NaN  NaN   \n",
      "2025-11-15 21:00:00                    NaN         NaN        NaN  NaN   \n",
      "2025-11-15 22:00:00                    NaN         NaN        NaN  NaN   \n",
      "2025-11-15 23:00:00                    NaN         NaN        NaN  NaN   \n",
      "2025-11-16 00:00:00                    NaN         NaN        NaN  NaN   \n",
      "2025-11-16 01:00:00                    NaN         NaN        NaN  NaN   \n",
      "2025-11-16 02:00:00                    NaN         NaN        NaN  NaN   \n",
      "2025-11-16 03:00:00                    NaN         NaN        NaN  NaN   \n",
      "2025-11-16 04:00:00                    NaN         NaN        NaN  NaN   \n",
      "2025-11-16 05:00:00                    NaN         NaN        NaN  NaN   \n",
      "\n",
      "                    load_area  mw is_verified  temp  hour  dayofweek  ...  \\\n",
      "2025-11-15 05:00:00      AECO NaN         NaN   6.0     5          5  ...   \n",
      "2025-11-15 06:00:00      AECO NaN         NaN   5.5     6          5  ...   \n",
      "2025-11-15 07:00:00      AECO NaN         NaN   6.7     7          5  ...   \n",
      "2025-11-15 08:00:00      AECO NaN         NaN   7.6     8          5  ...   \n",
      "2025-11-15 09:00:00      AECO NaN         NaN   7.9     9          5  ...   \n",
      "2025-11-15 10:00:00      AECO NaN         NaN   5.5    10          5  ...   \n",
      "2025-11-15 11:00:00      AECO NaN         NaN   3.1    11          5  ...   \n",
      "2025-11-15 12:00:00      AECO NaN         NaN   3.8    12          5  ...   \n",
      "2025-11-15 13:00:00      AECO NaN         NaN   5.8    13          5  ...   \n",
      "2025-11-15 14:00:00      AECO NaN         NaN   7.9    14          5  ...   \n",
      "2025-11-15 15:00:00      AECO NaN         NaN   8.9    15          5  ...   \n",
      "2025-11-15 16:00:00      AECO NaN         NaN  11.1    16          5  ...   \n",
      "2025-11-15 17:00:00      AECO NaN         NaN  12.6    17          5  ...   \n",
      "2025-11-15 18:00:00      AECO NaN         NaN  13.1    18          5  ...   \n",
      "2025-11-15 19:00:00      AECO NaN         NaN  12.7    19          5  ...   \n",
      "2025-11-15 20:00:00      AECO NaN         NaN  12.6    20          5  ...   \n",
      "2025-11-15 21:00:00      AECO NaN         NaN  12.3    21          5  ...   \n",
      "2025-11-15 22:00:00      AECO NaN         NaN  11.8    22          5  ...   \n",
      "2025-11-15 23:00:00      AECO NaN         NaN  11.5    23          5  ...   \n",
      "2025-11-16 00:00:00      AECO NaN         NaN  12.3     0          6  ...   \n",
      "2025-11-16 01:00:00      AECO NaN         NaN  12.6     1          6  ...   \n",
      "2025-11-16 02:00:00      AECO NaN         NaN  13.1     2          6  ...   \n",
      "2025-11-16 03:00:00      AECO NaN         NaN  13.8     3          6  ...   \n",
      "2025-11-16 04:00:00      AECO NaN         NaN  14.2     4          6  ...   \n",
      "2025-11-16 05:00:00      AECO NaN         NaN  15.1     5          6  ...   \n",
      "\n",
      "                     temp_lag_216  temp_lag_240  mw_lag_72  mw_lag_96  \\\n",
      "2025-11-15 05:00:00          12.9           7.0        NaN        NaN   \n",
      "2025-11-15 06:00:00          13.6           6.0        NaN        NaN   \n",
      "2025-11-15 07:00:00          12.0           4.0        NaN        NaN   \n",
      "2025-11-15 08:00:00          12.0           5.0        NaN        NaN   \n",
      "2025-11-15 09:00:00          10.0           4.0        NaN        NaN   \n",
      "2025-11-15 10:00:00           9.0           2.0        NaN        NaN   \n",
      "2025-11-15 11:00:00           9.0           2.0        NaN        NaN   \n",
      "2025-11-15 12:00:00          10.0           7.0        NaN        NaN   \n",
      "2025-11-15 13:00:00          11.0           9.0        NaN        NaN   \n",
      "2025-11-15 14:00:00          12.0          16.0        NaN        NaN   \n",
      "2025-11-15 15:00:00          13.0          17.0        NaN        NaN   \n",
      "2025-11-15 16:00:00          13.0          19.0        NaN        NaN   \n",
      "2025-11-15 17:00:00          13.0          20.0        NaN        NaN   \n",
      "2025-11-15 18:00:00          13.0          19.0        NaN        NaN   \n",
      "2025-11-15 19:00:00          13.0          18.0        NaN        NaN   \n",
      "2025-11-15 20:00:00          13.0          18.0        NaN        NaN   \n",
      "2025-11-15 21:00:00           9.0          17.0        NaN        NaN   \n",
      "2025-11-15 22:00:00          10.0          17.0        NaN        NaN   \n",
      "2025-11-15 23:00:00           6.0          16.0        NaN        NaN   \n",
      "2025-11-16 00:00:00           6.0          17.0        NaN        NaN   \n",
      "2025-11-16 01:00:00           4.0          17.0        NaN        NaN   \n",
      "2025-11-16 02:00:00           1.0          18.0        NaN        NaN   \n",
      "2025-11-16 03:00:00           1.0          18.0        NaN        NaN   \n",
      "2025-11-16 04:00:00           1.0          17.0        NaN        NaN   \n",
      "2025-11-16 05:00:00           1.0          12.9        NaN        NaN   \n",
      "\n",
      "                     mw_lag_120  mw_lag_144  mw_lag_168  mw_lag_192  \\\n",
      "2025-11-15 05:00:00         NaN     771.626     787.565     847.042   \n",
      "2025-11-15 06:00:00         NaN     744.242     755.219     826.556   \n",
      "2025-11-15 07:00:00         NaN     727.425     740.227     815.267   \n",
      "2025-11-15 08:00:00         NaN     724.495     724.307     817.725   \n",
      "2025-11-15 09:00:00         NaN     725.101     726.972     835.460   \n",
      "2025-11-15 10:00:00         NaN     741.362     744.177     873.884   \n",
      "2025-11-15 11:00:00         NaN     761.900     763.911     929.639   \n",
      "2025-11-15 12:00:00         NaN     767.453     777.114     917.034   \n",
      "2025-11-15 13:00:00         NaN     798.917     716.112     815.222   \n",
      "2025-11-15 14:00:00         NaN     806.536     633.057     736.582   \n",
      "2025-11-15 15:00:00         NaN     852.446     575.547     658.874   \n",
      "2025-11-15 16:00:00         NaN     820.682     548.385     601.289   \n",
      "2025-11-15 17:00:00         NaN     848.334     553.877     624.328   \n",
      "2025-11-15 18:00:00         NaN     845.133     605.952     712.425   \n",
      "2025-11-15 19:00:00         NaN     825.526     684.010     792.055   \n",
      "2025-11-15 20:00:00         NaN     878.032     797.475     874.783   \n",
      "2025-11-15 21:00:00         NaN     948.480     908.032     959.621   \n",
      "2025-11-15 22:00:00         NaN    1017.033     963.920    1029.161   \n",
      "2025-11-15 23:00:00         NaN    1007.125     953.000    1020.872   \n",
      "2025-11-16 00:00:00         NaN     980.590     931.676     998.510   \n",
      "2025-11-16 01:00:00         NaN     952.221     910.315     975.784   \n",
      "2025-11-16 02:00:00         NaN     908.209     880.234     935.734   \n",
      "2025-11-16 03:00:00         NaN     855.014     846.536     888.583   \n",
      "2025-11-16 04:00:00         NaN     799.135     804.464     832.976   \n",
      "2025-11-16 05:00:00         NaN         NaN     771.626     787.565   \n",
      "\n",
      "                     mw_lag_216  mw_lag_240  \n",
      "2025-11-15 05:00:00     777.294     812.397  \n",
      "2025-11-15 06:00:00     748.663     781.214  \n",
      "2025-11-15 07:00:00     734.596     771.327  \n",
      "2025-11-15 08:00:00     733.044     775.366  \n",
      "2025-11-15 09:00:00     745.615     786.842  \n",
      "2025-11-15 10:00:00     780.537     840.617  \n",
      "2025-11-15 11:00:00     850.977     909.102  \n",
      "2025-11-15 12:00:00     859.701     902.434  \n",
      "2025-11-15 13:00:00     769.897     847.743  \n",
      "2025-11-15 14:00:00     666.715     704.103  \n",
      "2025-11-15 15:00:00     618.998     613.858  \n",
      "2025-11-15 16:00:00     623.023     577.680  \n",
      "2025-11-15 17:00:00     632.951     617.875  \n",
      "2025-11-15 18:00:00     660.835     705.602  \n",
      "2025-11-15 19:00:00     725.395     762.544  \n",
      "2025-11-15 20:00:00     815.896     855.402  \n",
      "2025-11-15 21:00:00     916.267     930.976  \n",
      "2025-11-15 22:00:00    1027.385     996.723  \n",
      "2025-11-15 23:00:00    1040.148    1025.043  \n",
      "2025-11-16 00:00:00    1030.297    1004.697  \n",
      "2025-11-16 01:00:00    1013.396     972.899  \n",
      "2025-11-16 02:00:00     971.824     933.132  \n",
      "2025-11-16 03:00:00     931.829     873.664  \n",
      "2025-11-16 04:00:00     888.421     820.733  \n",
      "2025-11-16 05:00:00     847.042     777.294  \n",
      "\n",
      "[25 rows x 38 columns]\n"
     ]
    }
   ],
   "source": [
    "# This function gets the EPT midnight of today, and expresses it as UTC\n",
    "def getMidnightToday():\n",
    "    #now = datetime.datetime.now()\n",
    "    #return now.replace(hour=0, minute=0, second=0, microsecond=0)\n",
    "    # Get current time in EPT (EST/EDT as appropriate)\n",
    "    now_ept = datetime.datetime.now(zoneinfo.ZoneInfo(\"America/New_York\"))\n",
    "    midnight_ept = now_ept.replace(hour=0, minute=0, second=0, microsecond=0)\n",
    "    midnight_utc = midnight_ept.astimezone(zoneinfo.ZoneInfo(\"UTC\"))\n",
    "    # Convert the aware datetime to a naive datetime\n",
    "    midnight_utc = midnight_utc.replace(tzinfo=None)\n",
    "    return midnight_utc\n",
    "\n",
    "# This function gets the EPT midnight of tomorrow, and expresses it as UTC\n",
    "def getMidnightTomorrow():\n",
    "    #tmrw = datetime.datetime.now() + datetime.timedelta(days=1)\n",
    "    #return tmrw.replace(hour=0, minute=0, second=0, microsecond=0)\n",
    "    tmrw_ept = datetime.datetime.now(zoneinfo.ZoneInfo(\"America/New_York\")) + datetime.timedelta(days=1)\n",
    "    midnight_ept = tmrw_ept.replace(hour=0, minute=0, second=0, microsecond=0)\n",
    "    midnight_utc = midnight_ept.astimezone(zoneinfo.ZoneInfo(\"UTC\"))\n",
    "    # Convert the aware datetime to a naive datetime\n",
    "    midnight_utc = midnight_utc.replace(tzinfo=None)\n",
    "    return midnight_utc\n",
    "\n",
    "def getPredictionFeatures(load_area):\n",
    "\n",
    "    energy_df = getEnergyDf(load_area)\n",
    "    weather_df = getWeatherDf(load_area, force_refresh=True)\n",
    "    prediction_df = add_features(energy_df, weather_df, dropna=False, outer=True)\n",
    "\n",
    "    start = getMidnightToday()\n",
    "    end = getMidnightTomorrow()\n",
    "    prediction_df = prediction_df.loc[start:end]\n",
    "    prediction_df[\"load_area\"] = load_area\n",
    "    return prediction_df\n",
    "\n",
    "aeco_prediction_features = getPredictionFeatures(\"AECO\")\n",
    "print(aeco_prediction_features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "7761281d-f27f-48b6-8a7a-3859cf20f353",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-11-15 05:00:00\n",
      "2025-11-16 05:00:00\n"
     ]
    }
   ],
   "source": [
    "print(getMidnightToday())\n",
    "print(getMidnightTomorrow())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "679550b7-6be7-4ffa-bcac-6763d05ba8e0",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
