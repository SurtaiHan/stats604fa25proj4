{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "4ee94787-26b0-4063-9236-685cee44f84f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'usaf = \"722950\"\\nwban = \"23174\"\\nyear = \"2024\"\\n\\nurl = f\"https://www.ncei.noaa.gov/pub/data/noaa/{year}/{usaf}-{wban}-{year}.gz\"\\nresponse = requests.get(url)\\n\\nwith open(\"station-raw.gz\", \"wb\") as f:\\n    f.write(response.content)\\n\\nrecords = isdparser.ISDParser(\"station-raw.gz\").records\\ndf = pd.DataFrame(records)\\ndf.head()\\n\\nstations = pd.read_csv(\"https://www.ncei.noaa.gov/pub/data/noaa/isd-history.csv\")\\n# Filter to stations of interest (e.g., by ICAO code like KPHL, KBWI, KCMH etc.)\\nstations = stations[stations[\\'ICAO\\'] == \\'KCMH\\']\\nprint(stations)\\n\\nyears = range(2024, 2025)\\nfor y in years:\\n    url = f\"https://www.ncei.noaa.gov/pub/data/noaa/{y}/{usaf}-{wban}-{y}.gz\"\\n    r = requests.get(url)\\n    open(f\"{usaf}-{wban}-{y}.gz\", \"wb\").write(r.content)'"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import requests\n",
    "import gzip\n",
    "import pgeocode\n",
    "#from isdparser import isdparser\n",
    "from meteostat import Point, Hourly\n",
    "#from datetime import datetime\n",
    "import datetime\n",
    "\n",
    "\"\"\"usaf = \"722950\"\n",
    "wban = \"23174\"\n",
    "year = \"2024\"\n",
    "\n",
    "url = f\"https://www.ncei.noaa.gov/pub/data/noaa/{year}/{usaf}-{wban}-{year}.gz\"\n",
    "response = requests.get(url)\n",
    "\n",
    "with open(\"station-raw.gz\", \"wb\") as f:\n",
    "    f.write(response.content)\n",
    "\n",
    "records = isdparser.ISDParser(\"station-raw.gz\").records\n",
    "df = pd.DataFrame(records)\n",
    "df.head()\n",
    "\n",
    "stations = pd.read_csv(\"https://www.ncei.noaa.gov/pub/data/noaa/isd-history.csv\")\n",
    "# Filter to stations of interest (e.g., by ICAO code like KPHL, KBWI, KCMH etc.)\n",
    "stations = stations[stations['ICAO'] == 'KCMH']\n",
    "print(stations)\n",
    "\n",
    "years = range(2024, 2025)\n",
    "for y in years:\n",
    "    url = f\"https://www.ncei.noaa.gov/pub/data/noaa/{y}/{usaf}-{wban}-{y}.gz\"\n",
    "    r = requests.get(url)\n",
    "    open(f\"{usaf}-{wban}-{y}.gz\", \"wb\").write(r.content)\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "4ac3099e-53f6-4065-9c4d-9a3f73dc4790",
   "metadata": {},
   "outputs": [],
   "source": [
    "def getWeatherFilePath(load_area):\n",
    "    filepath = \"data/weather/\" + load_area + \".csv\"\n",
    "    return filepath\n",
    "\n",
    "def getPjmFilePath(year):\n",
    "    filepath = \"data/pjm/\" + \"hrl_load_metered_\" + str(year) + \".csv\"\n",
    "    return filepath\n",
    "\n",
    "def getCompleteDfFilePath(load_area):\n",
    "    filepath = \"data/complete_dfs/\" + load_area + \".csv\"\n",
    "    return filepath"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "ac4b9608-161c-48b4-9c73-d417dab88111",
   "metadata": {},
   "outputs": [],
   "source": [
    "# RTO is the *entire PJM footprint*, not a load zone. no meaningful ZIP.\n",
    "zone_to_zips = {\n",
    "    # Atlantic City Electric (Southern New Jersey)\n",
    "    'AECO': ['08401'],   # Atlantic City, NJ\n",
    "    \n",
    "    # American Electric Power - Appalachian Power (central and Southern West Virginia)\n",
    "    'AEPAPT': ['25301'],   # Charleston, WV\n",
    "\n",
    "    # American Electric Power - Indiana Michigan Power (northeast quadrant of indiana and southwest corner of michigan)\n",
    "    'AEPIMP': ['46802'],   # Fort Wayne, IN\n",
    "\n",
    "    # American Electric Power - Kentucky Power (eastern kentucky)\n",
    "    'AEPKPT': ['41101'],   # Ashland, KY (Eastern Kentucky Power region)\n",
    "\n",
    "    # American Electric Power - Ohio (central and southeast ohio)\n",
    "    'AEPOPT': ['43215'],   # Columbus, OH\n",
    "\n",
    "    # Allegheny Power (FirstEnergy West) serving MD/WV/PA panhandle\n",
    "    'AP': ['21502'],     # Cumberland, MD\n",
    "\n",
    "    # Baltimore Gas & Electric\n",
    "    'BC': ['21201'],     # Baltimore, MD (city center)\n",
    "\n",
    "    # Cleveland Electric Illuminating Company (FirstEnergy)\n",
    "    'CE': ['44114'],     # Cleveland, OH (downtown)\n",
    "\n",
    "    # Dayton Power & Light (AES Ohio)\n",
    "    'DAY': ['45402'],    # Dayton, OH\n",
    "\n",
    "    # Duke Energy Ohio/Kentucky load zone\n",
    "    'DEOK': ['45202'],   # Cincinnati, OH\n",
    "\n",
    "    # Dominion Virginia Power\n",
    "    'DOM': ['23219'],    # Richmond, VA\n",
    "\n",
    "    # Delmarva Power (Delaware & Eastern Shore MD)\n",
    "    'DPLCO': ['19901'],  # Dover, DE\n",
    "\n",
    "    # Duquesne Light\n",
    "    'DUQ': ['15222'],    # Pittsburgh, PA\n",
    "\n",
    "    # Easton Utilities (Maryland municipal)\n",
    "    'EASTON': ['21601'], # Easton, MD\n",
    "\n",
    "    # East Kentucky Power Cooperative\n",
    "    'EKPC': ['40391'],   # Winchester, KY\n",
    "\n",
    "    # Jersey Central Power & Light (FirstEnergy NJ)\n",
    "    'JC': ['07728'],     # Freehold, NJ\n",
    "\n",
    "    # Metropolitan Edison (FirstEnergy PA)\n",
    "    'ME': ['19601'],     # Reading, PA\n",
    "\n",
    "    # Ohio Edison (FirstEnergy OH)\n",
    "    'OE': ['44308'],     # Akron, OH\n",
    "\n",
    "    # Ohio Valley Electric Corporation\n",
    "    'OVEC': ['45661'],   # Piketon, OH\n",
    "\n",
    "    # Pennsylvania Power Company (FirstEnergy PA)\n",
    "    'PAPWR': ['16101'],  # New Castle, PA\n",
    "\n",
    "    # PECO (Philadelphia)\n",
    "    'PE': ['19103'],     # Philadelphia, PA (Center City)\n",
    "\n",
    "    # Potomac Electric Power Company (Washington DC + Montgomery Co MD)\n",
    "    'PEPCO': ['20001'],  # Washington, DC\n",
    "\n",
    "    # Potomac Edison (FirstEnergy MD/WV)\n",
    "    'PLCO': ['21740'],   # Hagerstown, MD\n",
    "\n",
    "    # Pennsylvania Electric Company (Penelec - FirstEnergy Northwest/Central PA)\n",
    "    'PN': ['16601'],     # Altoona, PA\n",
    "\n",
    "    # Public Service Electric & Gas (PSE&G NJ)\n",
    "    'PS': ['07102'],     # Newark, NJ\n",
    "\n",
    "    # Rockland Electric (Northern NJ / small NY portion)\n",
    "    'RECO': ['07450'],   # Ridgewood, NJ\n",
    "\n",
    "    # Southern Maryland Electric Cooperative\n",
    "    'SMECO': ['20650'],  # Leonardtown, MD\n",
    "\n",
    "    # UGI Electric (NE Pennsylvania, Luzerne County)\n",
    "    'UGI': ['18702'],    # Wilkes-Barre, PA\n",
    "\n",
    "    # VMEU = Virginia Municipal Electric Utility (multiple small cities)\n",
    "    'VMEU': ['22801']    # Harrisonburg, VA (representative muni)\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "0e712946-e0cc-4f56-88af-d4d864aac759",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished checking inconsistencies!\n"
     ]
    }
   ],
   "source": [
    "for year in range(2018,2026):\n",
    "    df = pd.read_csv(getPjmFilePath(year))\n",
    "    #print(df['zone'].unique())\n",
    "    #print(df['load_area'].unique())\n",
    "    #print(len(df['load_area'].unique()))\n",
    "    set1 = set(df['load_area'].unique())\n",
    "    set2 = set(zone_to_zips.keys())\n",
    "    set2.add(\"RTO\")\n",
    "    # Note that years 2016 and 2017 do not match. Thus, we will start with 2018 inclusive\n",
    "    if len(set1.symmetric_difference(set2)) > 0:\n",
    "        print(str(year) + \" did not match! The difference is: \" + str(set1.symmetric_difference(set2)))\n",
    "    \n",
    "print(\"Finished checking inconsistencies!\")\n",
    "years = range(2018,2026)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "id": "c64fff20-540f-4fcb-95db-dce506a1c8b5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cached weather file for AECO was found\n",
      "Cached weather file for AEPAPT was found\n",
      "Cached weather file for AEPIMP was found\n",
      "Cached weather file for AEPKPT was found\n",
      "Cached weather file for AEPOPT was found\n",
      "Cached weather file for AP was found\n",
      "Cached weather file for BC was found\n",
      "Cached weather file for CE was found\n",
      "Cached weather file for DAY was found\n",
      "Cached weather file for DEOK was found\n",
      "Cached weather file for DOM was found\n",
      "Cached weather file for DPLCO was found\n",
      "Cached weather file for DUQ was found\n",
      "Cached weather file for EASTON was found\n",
      "Cached weather file for EKPC was found\n",
      "Cached weather file for JC was found\n",
      "Cached weather file for ME was found\n",
      "Cached weather file for OE was found\n",
      "Cached weather file for OVEC was found\n",
      "Cached weather file for PAPWR was found\n",
      "Cached weather file for PE was found\n",
      "Cached weather file for PEPCO was found\n",
      "Cached weather file for PLCO was found\n",
      "Cached weather file for PN was found\n",
      "Cached weather file for PS was found\n",
      "Cached weather file for RECO was found\n",
      "Cached weather file for SMECO was found\n",
      "Cached weather file for UGI was found\n",
      "Cached weather file for VMEU was found\n",
      "Cached weather file for AECO was found\n",
      "                     temp  dwpt  rhum  prcp  snow   wdir  wspd  wpgt    pres  \\\n",
      "time                                                                           \n",
      "2018-01-01 00:00:00 -10.6 -17.7  56.0   NaN   NaN  320.0  18.4   NaN  1027.6   \n",
      "2018-01-01 01:00:00 -12.2 -17.8  63.0   0.0   NaN  310.0  14.8   NaN  1027.5   \n",
      "2018-01-01 02:00:00 -12.8 -18.4  63.0   0.0   NaN  300.0  11.2   NaN  1027.7   \n",
      "2018-01-01 03:00:00 -12.8 -18.4  63.0   0.0   NaN  290.0   9.4   NaN  1028.1   \n",
      "2018-01-01 04:00:00 -13.9 -18.9  66.0   0.0   NaN  310.0  11.2   NaN  1028.3   \n",
      "\n",
      "                     tsun  coco  \n",
      "time                             \n",
      "2018-01-01 00:00:00   NaN   NaN  \n",
      "2018-01-01 01:00:00   NaN   NaN  \n",
      "2018-01-01 02:00:00   NaN   NaN  \n",
      "2018-01-01 03:00:00   NaN   NaN  \n",
      "2018-01-01 04:00:00   NaN   NaN  \n"
     ]
    }
   ],
   "source": [
    "# Create a Nominatim geocoder instance for the desired country (For the USA, use 'us')\n",
    "nomi = pgeocode.Nominatim('us')\n",
    "years = range(2018,2026)\n",
    "\n",
    "def getWeatherDf(load_area, force_refresh=False):\n",
    "    # First check if we have the information cached\n",
    "    file_path = getWeatherFilePath(load_area)\n",
    "    if os.path.exists(file_path) and not force_refresh:\n",
    "        temp = pd.read_csv(file_path)\n",
    "        #print(temp.head())\n",
    "        temp['time'] = pd.to_datetime(temp['time'])\n",
    "        temp = temp.set_index('time')\n",
    "        #print(temp.head())\n",
    "        if temp.index.min().year == years[0] and temp.index.max().year == years[-1]:\n",
    "            print(\"Cached weather file for \" + load_area + \" was found\")\n",
    "            return temp\n",
    "\n",
    "    # Query the postal code\n",
    "    zip_code = value[0]\n",
    "    location_data = nomi.query_postal_code(zip_code)\n",
    "    latitude = float(location_data.latitude)\n",
    "    longitude = float(location_data.longitude)\n",
    "    #print(latitude)\n",
    "    #print(longitude)\n",
    "\n",
    "    location = Point(latitude, longitude)\n",
    "    start = datetime.datetime(years[0], 1, 1)\n",
    "    end   = datetime.datetime(years[-1], 12, 31)\n",
    "\n",
    "    data = Hourly(location, start, end).fetch()\n",
    "    #print(data.head())\n",
    "    data.to_csv(getWeatherFilePath(load_area))\n",
    "    print(\"Added new cached weather file for \" + load_area)\n",
    "    return data\n",
    "\n",
    "\"\"\"def getForecastWeatherDf(load_area, datetime_obj):\n",
    "    # Query the postal code\n",
    "    zip_code = value[0]\n",
    "    location_data = nomi.query_postal_code(zip_code)\n",
    "    latitude = float(location_data.latitude)\n",
    "    longitude = float(location_data.longitude)\n",
    "    location = Point(latitude, longitude)\n",
    "    start = datetime_obj\n",
    "    end   = datetime_obj + datetime.timedelta(days=1)\n",
    "    data = Hourly(location, start, end).fetch()\n",
    "    return data\"\"\"\n",
    "\n",
    "\n",
    "for key, value in zone_to_zips.items():\n",
    "    load_area = key\n",
    "    getWeatherDf(load_area)\n",
    "\n",
    "aeco_weather_df = getWeatherDf(\"AECO\")\n",
    "print(aeco_weather_df.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "9fbd4772-f843-4f25-8d35-a1eaf69a0aeb",
   "metadata": {},
   "outputs": [],
   "source": [
    "import holidays\n",
    "#from datetime import timedelta, date\n",
    "\n",
    "us_holidays = holidays.US(years=years)\n",
    "\n",
    "# Add black friday\n",
    "for year in years:\n",
    "    thanksgiving = [day for day, name in us_holidays.items() if name == \"Thanksgiving Day\" and day.year == year][0]\n",
    "    us_holidays[thanksgiving + datetime.timedelta(days=1)] = \"Black Friday\"\n",
    "    us_holidays[thanksgiving - datetime.timedelta(days=1)] = \"Thanksgiving Eve\"\n",
    "\n",
    "#print(us_holidays)\n",
    "#print(us_holidays.keys())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "3ac9f6cb-8c14-47e7-96ff-6b3bce36c9c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def isHoliday(datetime_obj):\n",
    "     return datetime_obj.date() in us_holidays\n",
    "def isThanksgiving(datetime_obj):\n",
    "    return us_holidays.get(datetime_obj.date()) == \"Thanksgiving Day\"\n",
    "def isThanksgivingEve(datetime_obj):\n",
    "    return us_holidays.get(datetime_obj.date()) == \"Thanksgiving Eve\"\n",
    "def isBlackFriday(datetime_obj):\n",
    "    return us_holidays.get(datetime_obj.date()) == \"Black Friday\"\n",
    "def isWeekend(datetime_obj):\n",
    "    weekno = datetime.datetime.today().weekday()\n",
    "    if weekno < 5:\n",
    "        return False\n",
    "    return True\n",
    "\n",
    "# Thanksgiving 2024 (Nov 28, 2024)\n",
    "#test_dt = datetime.datetime(2024, 11, 29, 15, 0)  # 3pm on Thanksgiving Day\n",
    "#print(isHoliday(test_dt))\n",
    "#print(isBlackFriday(test_dt))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "f6db3b5f-52b8-48c5-a251-3a6afb8026b9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                       datetime_beginning_ept nerc_region mkt_region zone  \\\n",
      "datetime_beginning_utc                                                      \n",
      "2018-01-01 05:00:00      1/1/2018 12:00:00 AM         RFC     MIDATL   AE   \n",
      "2018-01-01 06:00:00       1/1/2018 1:00:00 AM         RFC     MIDATL   AE   \n",
      "2018-01-01 07:00:00       1/1/2018 2:00:00 AM         RFC     MIDATL   AE   \n",
      "2018-01-01 08:00:00       1/1/2018 3:00:00 AM         RFC     MIDATL   AE   \n",
      "2018-01-01 09:00:00       1/1/2018 4:00:00 AM         RFC     MIDATL   AE   \n",
      "\n",
      "                       load_area        mw  is_verified  \n",
      "datetime_beginning_utc                                   \n",
      "2018-01-01 05:00:00         AECO  1261.317         True  \n",
      "2018-01-01 06:00:00         AECO  1221.330         True  \n",
      "2018-01-01 07:00:00         AECO  1190.517         True  \n",
      "2018-01-01 08:00:00         AECO  1174.050         True  \n",
      "2018-01-01 09:00:00         AECO  1168.370         True  \n"
     ]
    }
   ],
   "source": [
    "# Load PJM data\n",
    "\n",
    "def getEnergyDf(load_area):\n",
    "    if load_area not in zone_to_zips.keys():\n",
    "        print(\"Error: load_area(\" + load_area + \") not found!\")\n",
    "        return -1\n",
    "\n",
    "    ret_df = None\n",
    "    for year in years:\n",
    "        pjm_df = pd.read_csv(getPjmFilePath(year))\n",
    "        pjm_df = pjm_df[pjm_df['load_area'] == load_area]\n",
    "        \n",
    "        if ret_df is None:\n",
    "            ret_df = pjm_df\n",
    "        else:\n",
    "            ret_df = pd.concat([ret_df, pjm_df])\n",
    "\n",
    "    ret_df['datetime_beginning_utc'] = pd.to_datetime(\n",
    "        ret_df['datetime_beginning_utc'], \n",
    "        #utc=True\n",
    "    )\n",
    "    ret_df = ret_df.set_index('datetime_beginning_utc')\n",
    "    ret_df = ret_df.sort_index()   # Always a good idea after setting index\n",
    "\n",
    "    return ret_df\n",
    "\n",
    "aeco_energy_df = getEnergyDf(\"AECO\")\n",
    "print(aeco_energy_df.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "id": "e337a1c9-cf6d-4e5b-9b74-bd1ac360216a",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Last step is to augment the pjm data with:\n",
    "# 1) weather at that current time  (we just use the ground truth weather)\n",
    "# 2) previous day's weather\n",
    "# 3) isHoliday, isThanksgiving, isThanksgivingEve, isBlackFriday, isWeekend\n",
    "# 4) lagged loads (24 hrs ago, 168 hrs ago)\n",
    "\n",
    "# The resulting dataframe could be viewed as X,y tuples\n",
    "# in the sense that everything but \n",
    "\n",
    "# weather variables available:\n",
    "# temp\tdwpt\trhum\tprcp\tsnow\twdir\twspd\twpgt\tpres\ttsun\tcoco\n",
    "\n",
    "def add_features(energy_df, weather_df, dropna=True, outer=False):\n",
    "    \"\"\"\n",
    "    df must contain at least:\n",
    "    - load (float)\n",
    "    - temp, dwpt, rhum, wspd, tsun, etc. (weather columns)\n",
    "    - index is hourly timestamps (pd.DatetimeIndex)\n",
    "    \"\"\"\n",
    "\n",
    "    \"\"\"df = df.merge(\n",
    "    weather_df[['temp']], # Only select 'temp', as the timestamp is the index\n",
    "    left_on='datetime_beginning_utc',\n",
    "    right_index=True,   # Tells pandas to use the index of weather_df (the DatetimeIndex)\n",
    "    how='left'\n",
    "    )\"\"\"\n",
    "    df = energy_df.copy()\n",
    "\n",
    "    if outer:\n",
    "        df = df.join(weather_df[['temp']], how='outer')\n",
    "    else:\n",
    "        df = df.join(weather_df[['temp']], how='left')\n",
    "    \n",
    "    # --- Calendar Features ---\n",
    "    df['hour'] = df.index.hour\n",
    "    df['dayofweek'] = df.index.dayofweek\n",
    "    df['month'] = df.index.month\n",
    "    df['is_weekend'] = df['dayofweek'].isin([5,6]).astype(int)\n",
    "\n",
    "    # Holiday features (assuming you already defined isHoliday / isThanksgiving):\n",
    "    df['is_holiday'] = df.index.to_series().apply(isHoliday)\n",
    "    df['is_thanksgiving'] = df.index.to_series().apply(isThanksgiving)\n",
    "    df['is_thanksgiving_eve'] = df.index.to_series().apply(isThanksgivingEve)\n",
    "    df['is_black_friday'] = df.index.to_series().apply(isBlackFriday)\n",
    "\n",
    "    # --- Degree Days ---\n",
    "    df['HDD'] = (18 - df['temp']).clip(lower=0)\n",
    "    df['CDD'] = (df['temp'] - 18).clip(lower=0)\n",
    "\n",
    "    # --- Rolling Weather Averages ---\n",
    "    df['temp_rolling_24'] = df['temp'].rolling('24h', min_periods=1).mean()\n",
    "    # df['dwpt_rolling_24'] = df['dwpt'].rolling(24, min_periods=1).mean()\n",
    "\n",
    "    # --- Lagged Weather Features ---\n",
    "    df['temp_lag_24'] = df['temp'].shift(freq='24h')\n",
    "    df['temp_lag_168'] = df['temp'].shift(freq='168h')\n",
    "\n",
    "    # --- Lagged Load Features (Key Predictors) ---\n",
    "    df['mw_lag_24'] = df['mw'].shift(freq='24h')\n",
    "    df['mw_lag_168'] = df['mw'].shift(freq='168h')\n",
    "\n",
    "    # Drop rows where lag features are missing\n",
    "    if dropna:\n",
    "        df = df.dropna()\n",
    "\n",
    "    return df\n",
    "\n",
    "#df_augmented = add_features(aeco_energy_df, aeco_weather_df, False)\n",
    "#print(df_augmented)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "id": "4d9ebbd7-f548-48a6-b624-c9ed431dd53e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cached complete file for AECO was found\n",
      "Cached complete file for AEPAPT was found\n",
      "Cached complete file for AEPIMP was found\n",
      "Cached complete file for AEPKPT was found\n",
      "Cached complete file for AEPOPT was found\n",
      "Cached complete file for AP was found\n",
      "Cached complete file for BC was found\n",
      "Cached complete file for CE was found\n",
      "Cached complete file for DAY was found\n",
      "Cached complete file for DEOK was found\n",
      "Cached complete file for DOM was found\n",
      "Cached complete file for DPLCO was found\n",
      "Cached complete file for DUQ was found\n",
      "Cached complete file for EASTON was found\n",
      "Cached complete file for EKPC was found\n",
      "Cached complete file for JC was found\n",
      "Cached complete file for ME was found\n",
      "Cached complete file for OE was found\n",
      "Cached complete file for OVEC was found\n",
      "Cached complete file for PAPWR was found\n",
      "Cached complete file for PE was found\n",
      "Cached complete file for PEPCO was found\n",
      "Cached complete file for PLCO was found\n",
      "Cached complete file for PN was found\n",
      "Cached complete file for PS was found\n",
      "Cached complete file for RECO was found\n",
      "Cached complete file for SMECO was found\n",
      "Cached complete file for UGI was found\n",
      "Cached complete file for VMEU was found\n"
     ]
    }
   ],
   "source": [
    "def getCompleteDf(load_area):\n",
    "    # First check if we have the information cached\n",
    "    file_path = getCompleteDfFilePath(load_area)\n",
    "    if os.path.exists(file_path):\n",
    "        temp = pd.read_csv(file_path)\n",
    "        temp['datetime_beginning_utc'] = pd.to_datetime(temp['datetime_beginning_utc'])\n",
    "        temp = temp.set_index('datetime_beginning_utc')\n",
    "        temp = temp.sort_index()\n",
    "        print(\"Cached complete file for \" + load_area + \" was found\")\n",
    "        return temp\n",
    "\n",
    "    energy_df = getEnergyDf(load_area)\n",
    "    weather_df = getWeatherDf(load_area)\n",
    "    df_augmented = add_features(energy_df, weather_df)\n",
    "    load_area_to_complete_df[load_area] = df_augmented\n",
    "    df_augmented.to_csv(getCompleteDfFilePath(load_area))\n",
    "    print(\"Added new cached complete file for \" + load_area)\n",
    "    return df_augmented\n",
    "\n",
    "def getAllCompleteDfs():\n",
    "    load_area_to_complete_df = {}\n",
    "    for load_area in zone_to_zips.keys():\n",
    "        load_area_to_complete_df[load_area] = getCompleteDf(load_area)\n",
    "\n",
    "# print(getCompleteDf(\"AECO\").head())\n",
    "\n",
    "load_area_to_complete_df = getAllCompleteDfs()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "id": "ab5fd67f-92d3-4f13-847f-cdd830738ad3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Added new cached weather file for AECO\n",
      "                    datetime_beginning_ept nerc_region mkt_region zone  \\\n",
      "2025-11-09 00:00:00                    NaN         NaN        NaN  NaN   \n",
      "2025-11-09 01:00:00                    NaN         NaN        NaN  NaN   \n",
      "2025-11-09 02:00:00                    NaN         NaN        NaN  NaN   \n",
      "2025-11-09 03:00:00                    NaN         NaN        NaN  NaN   \n",
      "2025-11-09 04:00:00                    NaN         NaN        NaN  NaN   \n",
      "2025-11-09 05:00:00                    NaN         NaN        NaN  NaN   \n",
      "2025-11-09 06:00:00                    NaN         NaN        NaN  NaN   \n",
      "2025-11-09 07:00:00                    NaN         NaN        NaN  NaN   \n",
      "2025-11-09 08:00:00                    NaN         NaN        NaN  NaN   \n",
      "2025-11-09 09:00:00                    NaN         NaN        NaN  NaN   \n",
      "2025-11-09 10:00:00                    NaN         NaN        NaN  NaN   \n",
      "2025-11-09 11:00:00                    NaN         NaN        NaN  NaN   \n",
      "2025-11-09 12:00:00                    NaN         NaN        NaN  NaN   \n",
      "2025-11-09 13:00:00                    NaN         NaN        NaN  NaN   \n",
      "2025-11-09 14:00:00                    NaN         NaN        NaN  NaN   \n",
      "2025-11-09 15:00:00                    NaN         NaN        NaN  NaN   \n",
      "2025-11-09 16:00:00                    NaN         NaN        NaN  NaN   \n",
      "2025-11-09 17:00:00                    NaN         NaN        NaN  NaN   \n",
      "2025-11-09 18:00:00                    NaN         NaN        NaN  NaN   \n",
      "2025-11-09 19:00:00                    NaN         NaN        NaN  NaN   \n",
      "2025-11-09 20:00:00                    NaN         NaN        NaN  NaN   \n",
      "2025-11-09 21:00:00                    NaN         NaN        NaN  NaN   \n",
      "2025-11-09 22:00:00                    NaN         NaN        NaN  NaN   \n",
      "2025-11-09 23:00:00                    NaN         NaN        NaN  NaN   \n",
      "2025-11-10 00:00:00                    NaN         NaN        NaN  NaN   \n",
      "\n",
      "                    load_area  mw is_verified  temp  hour  dayofweek  ...  \\\n",
      "2025-11-09 00:00:00      AECO NaN         NaN   9.2     0          6  ...   \n",
      "2025-11-09 01:00:00      AECO NaN         NaN   7.6     1          6  ...   \n",
      "2025-11-09 02:00:00      AECO NaN         NaN   6.2     2          6  ...   \n",
      "2025-11-09 03:00:00      AECO NaN         NaN   4.5     3          6  ...   \n",
      "2025-11-09 04:00:00      AECO NaN         NaN   4.1     4          6  ...   \n",
      "2025-11-09 05:00:00      AECO NaN         NaN   3.5     5          6  ...   \n",
      "2025-11-09 06:00:00      AECO NaN         NaN   2.8     6          6  ...   \n",
      "2025-11-09 07:00:00      AECO NaN         NaN   2.4     7          6  ...   \n",
      "2025-11-09 08:00:00      AECO NaN         NaN   2.0     8          6  ...   \n",
      "2025-11-09 09:00:00      AECO NaN         NaN   2.8     9          6  ...   \n",
      "2025-11-09 10:00:00      AECO NaN         NaN   3.3    10          6  ...   \n",
      "2025-11-09 11:00:00      AECO NaN         NaN   3.6    11          6  ...   \n",
      "2025-11-09 12:00:00      AECO NaN         NaN   3.5    12          6  ...   \n",
      "2025-11-09 13:00:00      AECO NaN         NaN   5.4    13          6  ...   \n",
      "2025-11-09 14:00:00      AECO NaN         NaN   9.5    14          6  ...   \n",
      "2025-11-09 15:00:00      AECO NaN         NaN  14.0    15          6  ...   \n",
      "2025-11-09 16:00:00      AECO NaN         NaN  16.6    16          6  ...   \n",
      "2025-11-09 17:00:00      AECO NaN         NaN  18.1    17          6  ...   \n",
      "2025-11-09 18:00:00      AECO NaN         NaN  18.4    18          6  ...   \n",
      "2025-11-09 19:00:00      AECO NaN         NaN  18.6    19          6  ...   \n",
      "2025-11-09 20:00:00      AECO NaN         NaN  17.6    20          6  ...   \n",
      "2025-11-09 21:00:00      AECO NaN         NaN  16.5    21          6  ...   \n",
      "2025-11-09 22:00:00      AECO NaN         NaN  14.3    22          6  ...   \n",
      "2025-11-09 23:00:00      AECO NaN         NaN  12.0    23          6  ...   \n",
      "2025-11-10 00:00:00      AECO NaN         NaN  10.3     0          0  ...   \n",
      "\n",
      "                     is_thanksgiving  is_thanksgiving_eve  is_black_friday  \\\n",
      "2025-11-09 00:00:00            False                False            False   \n",
      "2025-11-09 01:00:00            False                False            False   \n",
      "2025-11-09 02:00:00            False                False            False   \n",
      "2025-11-09 03:00:00            False                False            False   \n",
      "2025-11-09 04:00:00            False                False            False   \n",
      "2025-11-09 05:00:00            False                False            False   \n",
      "2025-11-09 06:00:00            False                False            False   \n",
      "2025-11-09 07:00:00            False                False            False   \n",
      "2025-11-09 08:00:00            False                False            False   \n",
      "2025-11-09 09:00:00            False                False            False   \n",
      "2025-11-09 10:00:00            False                False            False   \n",
      "2025-11-09 11:00:00            False                False            False   \n",
      "2025-11-09 12:00:00            False                False            False   \n",
      "2025-11-09 13:00:00            False                False            False   \n",
      "2025-11-09 14:00:00            False                False            False   \n",
      "2025-11-09 15:00:00            False                False            False   \n",
      "2025-11-09 16:00:00            False                False            False   \n",
      "2025-11-09 17:00:00            False                False            False   \n",
      "2025-11-09 18:00:00            False                False            False   \n",
      "2025-11-09 19:00:00            False                False            False   \n",
      "2025-11-09 20:00:00            False                False            False   \n",
      "2025-11-09 21:00:00            False                False            False   \n",
      "2025-11-09 22:00:00            False                False            False   \n",
      "2025-11-09 23:00:00            False                False            False   \n",
      "2025-11-10 00:00:00            False                False            False   \n",
      "\n",
      "                      HDD  CDD  temp_rolling_24  temp_lag_24  temp_lag_168  \\\n",
      "2025-11-09 00:00:00   8.8  0.0        12.891667         15.0           6.0   \n",
      "2025-11-09 01:00:00  10.4  0.0        12.583333         15.0           6.0   \n",
      "2025-11-09 02:00:00  11.8  0.0        12.216667         15.0           5.0   \n",
      "2025-11-09 03:00:00  13.5  0.0        11.820833         14.0           5.0   \n",
      "2025-11-09 04:00:00  13.9  0.0        11.491667         12.0           5.0   \n",
      "2025-11-09 05:00:00  14.5  0.0        11.137500         12.0           5.0   \n",
      "2025-11-09 06:00:00  15.2  0.0        10.754167         12.0           6.0   \n",
      "2025-11-09 07:00:00  15.6  0.0        10.395833         11.0           6.0   \n",
      "2025-11-09 08:00:00  16.0  0.0        10.062500         10.0           5.0   \n",
      "2025-11-09 09:00:00  15.2  0.0         9.845833          8.0           4.0   \n",
      "2025-11-09 10:00:00  14.7  0.0         9.691667          7.0           4.0   \n",
      "2025-11-09 11:00:00  14.4  0.0         9.633333          5.0           4.0   \n",
      "2025-11-09 12:00:00  14.5  0.0         9.570833          5.0           4.0   \n",
      "2025-11-09 13:00:00  12.6  0.0         9.504167          7.0           5.0   \n",
      "2025-11-09 14:00:00   8.5  0.0         9.441667         11.0           8.0   \n",
      "2025-11-09 15:00:00   4.0  0.0         9.400000         15.0          11.0   \n",
      "2025-11-09 16:00:00   1.4  0.0         9.341667         18.0          14.0   \n",
      "2025-11-09 17:00:00   0.0  0.1         9.295833         19.2          15.0   \n",
      "2025-11-09 18:00:00   0.0  0.4         9.250000         19.5          16.0   \n",
      "2025-11-09 19:00:00   0.0  0.6         9.245833         18.7          16.0   \n",
      "2025-11-09 20:00:00   0.4  0.0         9.195833         18.8          16.0   \n",
      "2025-11-09 21:00:00   1.5  0.0         9.129167         18.1          15.0   \n",
      "2025-11-09 22:00:00   3.7  0.0         9.083333         15.4          12.0   \n",
      "2025-11-09 23:00:00   6.0  0.0         9.020833         13.5          10.0   \n",
      "2025-11-10 00:00:00   7.7  0.0         9.066667          9.2           8.0   \n",
      "\n",
      "                     mw_lag_24  mw_lag_168  \n",
      "2025-11-09 00:00:00        NaN         NaN  \n",
      "2025-11-09 01:00:00        NaN         NaN  \n",
      "2025-11-09 02:00:00        NaN         NaN  \n",
      "2025-11-09 03:00:00        NaN         NaN  \n",
      "2025-11-09 04:00:00        NaN         NaN  \n",
      "2025-11-09 05:00:00        NaN         NaN  \n",
      "2025-11-09 06:00:00        NaN         NaN  \n",
      "2025-11-09 07:00:00        NaN         NaN  \n",
      "2025-11-09 08:00:00        NaN         NaN  \n",
      "2025-11-09 09:00:00        NaN         NaN  \n",
      "2025-11-09 10:00:00        NaN         NaN  \n",
      "2025-11-09 11:00:00        NaN         NaN  \n",
      "2025-11-09 12:00:00        NaN         NaN  \n",
      "2025-11-09 13:00:00        NaN         NaN  \n",
      "2025-11-09 14:00:00        NaN         NaN  \n",
      "2025-11-09 15:00:00        NaN         NaN  \n",
      "2025-11-09 16:00:00        NaN         NaN  \n",
      "2025-11-09 17:00:00        NaN         NaN  \n",
      "2025-11-09 18:00:00        NaN         NaN  \n",
      "2025-11-09 19:00:00        NaN         NaN  \n",
      "2025-11-09 20:00:00        NaN         NaN  \n",
      "2025-11-09 21:00:00        NaN         NaN  \n",
      "2025-11-09 22:00:00        NaN         NaN  \n",
      "2025-11-09 23:00:00        NaN         NaN  \n",
      "2025-11-10 00:00:00        NaN         NaN  \n",
      "\n",
      "[25 rows x 23 columns]\n"
     ]
    }
   ],
   "source": [
    "def getMidnightToday():\n",
    "    now = datetime.datetime.now()\n",
    "    return now.replace(hour=0, minute=0, second=0, microsecond=0)\n",
    "\n",
    "def getMidnightTomorrow():\n",
    "    tmrw = datetime.datetime.now() + datetime.timedelta(days=1)\n",
    "    return tmrw.replace(hour=0, minute=0, second=0, microsecond=0)\n",
    "\n",
    "def getPredictionFeatures(load_area):\n",
    "\n",
    "    energy_df = getEnergyDf(load_area)\n",
    "    weather_df = getWeatherDf(load_area, force_refresh=True)\n",
    "    prediction_df = add_features(energy_df, weather_df, dropna=False, outer=True)\n",
    "\n",
    "    start = getMidnightToday()\n",
    "    end = getMidnightTomorrow()\n",
    "    prediction_df = prediction_df.loc[start:end]\n",
    "    prediction_df[\"load_area\"] = load_area\n",
    "    return prediction_df\n",
    "\n",
    "# Note: The remaining issue is that the energy df that's given in canvas stops october 31. so we lose the mw predictors\n",
    "aeco_prediction_features = getPredictionFeatures(\"AECO\")\n",
    "print(aeco_prediction_features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "id": "7761281d-f27f-48b6-8a7a-3859cf20f353",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-11-09 00:00:00\n",
      "2025-11-10 00:00:00\n"
     ]
    }
   ],
   "source": [
    "print(getMidnightToday())\n",
    "print(getMidnightTomorrow())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "679550b7-6be7-4ffa-bcac-6763d05ba8e0",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
